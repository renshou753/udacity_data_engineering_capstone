{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import os\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "from pandas_profiling import ProfileReport\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env set up\n",
    "base_dir = os.path.abspath(os.getcwd())\n",
    "data_dir = os.path.join(base_dir, \"data_sources\")\n",
    "city_filepath = os.path.join(data_dir, \"us-cities-demographics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psql connection string\n",
    "SQLALCHAMY_DATABASE_URL = f\"postgresql://capstone:example@127.0.0.1:5434/capstone\"\n",
    "\n",
    "engine = create_engine(SQLALCHAMY_DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope & Describe Data\n",
    "\n",
    "In this project I plan to use I94 immigration dataset and US city demographic dataset to make a small size data warehouse for analyzing the immigrants coming to United State.\n",
    "\n",
    "I will adopt Kimball's dimensional modelling technique to make analytic tables for those two datasets. The analysis will starts with identifying the business process requirements, to extract and load the datasets into postgres database, then I will model the fact and dimension tables for further analysis.\n",
    "\n",
    "The primary tools I used in this project are python, sql and dbt. I use python to script the extraction and loading parts of the ELT workflow, for the data transformation I mainly use sql and dbt in conjunction with airflow to do the workflow scheduling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immi = pd.DataFrame(columns=['cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', 'i94port', 'arrdate',\n",
    "       'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa', 'count',\n",
    "       'dtadfile', 'visapost', 'occup', 'entdepa', 'entdepd', 'entdepu',\n",
    "       'matflag', 'biryear', 'dtaddto', 'gender', 'insnum', 'airline',\n",
    "       'admnum', 'fltno', 'visatype'])\n",
    "\n",
    "# loop through all parquet files\n",
    "for subdir, dirs, files in os.walk(data_dir):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "\n",
    "        if filepath.endswith(\".parquet\"):\n",
    "            df_temp = pd.read_parquet(filepath, engine='pyarrow')\n",
    "            df_immi = pd.concat([df_immi, df_temp])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immi['updated_at'] = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4904480.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>20570.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>VA</td>\n",
       "      <td>20583.0</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>10252016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.462202e+10</td>\n",
       "      <td>00262</td>\n",
       "      <td>B2</td>\n",
       "      <td>2022-12-11 13:18:58.006531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4904481.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>20570.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>VA</td>\n",
       "      <td>20583.0</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>10252016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.462196e+10</td>\n",
       "      <td>00262</td>\n",
       "      <td>B2</td>\n",
       "      <td>2022-12-11 13:18:58.006531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4904482.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>20570.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>VA</td>\n",
       "      <td>20583.0</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>10252016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.462200e+10</td>\n",
       "      <td>00262</td>\n",
       "      <td>B2</td>\n",
       "      <td>2022-12-11 13:18:58.006531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4904483.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>20570.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20580.0</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>1977.0</td>\n",
       "      <td>10252016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.462163e+10</td>\n",
       "      <td>00262</td>\n",
       "      <td>B1</td>\n",
       "      <td>2022-12-11 13:18:58.006531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4904490.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>20570.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>DE</td>\n",
       "      <td>20595.0</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>1963.0</td>\n",
       "      <td>10252016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.461481e+10</td>\n",
       "      <td>00188</td>\n",
       "      <td>B2</td>\n",
       "      <td>2022-12-11 13:18:58.006531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode  \\\n",
       "0  4904480.0  2016.0     4.0   245.0   245.0     CHI  20570.0      1.0   \n",
       "1  4904481.0  2016.0     4.0   245.0   245.0     CHI  20570.0      1.0   \n",
       "2  4904482.0  2016.0     4.0   245.0   245.0     CHI  20570.0      1.0   \n",
       "3  4904483.0  2016.0     4.0   245.0   245.0     CHI  20570.0      1.0   \n",
       "4  4904490.0  2016.0     4.0   245.0   245.0     CHI  20570.0      1.0   \n",
       "\n",
       "  i94addr  depdate  ...  matflag  biryear   dtaddto gender insnum airline  \\\n",
       "0      VA  20583.0  ...        M   1968.0  10252016      M   None      AA   \n",
       "1      VA  20583.0  ...        M   1971.0  10252016      F   None      AA   \n",
       "2      VA  20583.0  ...        M   1996.0  10252016      M   None      AA   \n",
       "3      WA  20580.0  ...        M   1977.0  10252016      F   None      AA   \n",
       "4      DE  20595.0  ...        M   1963.0  10252016      F   None      DL   \n",
       "\n",
       "         admnum  fltno visatype                 updated_at  \n",
       "0  9.462202e+10  00262       B2 2022-12-11 13:18:58.006531  \n",
       "1  9.462196e+10  00262       B2 2022-12-11 13:18:58.006531  \n",
       "2  9.462200e+10  00262       B2 2022-12-11 13:18:58.006531  \n",
       "3  9.462163e+10  00262       B1 2022-12-11 13:18:58.006531  \n",
       "4  9.461481e+10  00188       B2 2022-12-11 13:18:58.006531  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cicid                float64\n",
       "i94yr                float64\n",
       "i94mon               float64\n",
       "i94cit               float64\n",
       "i94res               float64\n",
       "i94port               object\n",
       "arrdate              float64\n",
       "i94mode              float64\n",
       "i94addr               object\n",
       "depdate              float64\n",
       "i94bir               float64\n",
       "i94visa              float64\n",
       "count                float64\n",
       "dtadfile              object\n",
       "visapost              object\n",
       "occup                 object\n",
       "entdepa               object\n",
       "entdepd               object\n",
       "entdepu               object\n",
       "matflag               object\n",
       "biryear              float64\n",
       "dtaddto               object\n",
       "gender                object\n",
       "insnum                object\n",
       "airline               object\n",
       "admnum               float64\n",
       "fltno                 object\n",
       "visatype              object\n",
       "updated_at    datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immi.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city = pd.read_csv(city_filepath, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['City', 'State', 'Median Age', 'Male Population', 'Female Population',\n",
       "       'Total Population', 'Number of Veterans', 'Foreign-born',\n",
       "       'Average Household Size', 'State Code', 'Race', 'Count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city.rename(columns={'City': 'city', 'State': 'state', 'Median Age': 'median_age', 'Male Population': 'male_population',\n",
    "                       'Female Population': 'female_population', 'Total Population': 'total_population', 'Number of Veterans': 'number_of_veterans', \n",
    "                       'Foreign-born': 'foreign_born', 'Average Household Size': 'average_household_size',\n",
    "                        'State Code': 'state_code', 'Race': 'race', 'Count': 'count'\n",
    "                       }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city['updated_at'] = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>median_age</th>\n",
       "      <th>male_population</th>\n",
       "      <th>female_population</th>\n",
       "      <th>total_population</th>\n",
       "      <th>number_of_veterans</th>\n",
       "      <th>foreign_born</th>\n",
       "      <th>average_household_size</th>\n",
       "      <th>state_code</th>\n",
       "      <th>race</th>\n",
       "      <th>count</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "      <td>2022-12-11 13:19:02.219098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "      <td>2022-12-11 13:19:02.219098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "      <td>2022-12-11 13:19:02.219098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "      <td>2022-12-11 13:19:02.219098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "      <td>2022-12-11 13:19:02.219098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               city          state  median_age  male_population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   female_population  total_population  number_of_veterans  foreign_born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   average_household_size state_code                       race  count  \\\n",
       "0                    2.60         MD         Hispanic or Latino  25924   \n",
       "1                    2.39         MA                      White  58723   \n",
       "2                    2.58         AL                      Asian   4759   \n",
       "3                    3.18         CA  Black or African-American  24437   \n",
       "4                    2.73         NJ                      White  76402   \n",
       "\n",
       "                  updated_at  \n",
       "0 2022-12-11 13:19:02.219098  \n",
       "1 2022-12-11 13:19:02.219098  \n",
       "2 2022-12-11 13:19:02.219098  \n",
       "3 2022-12-11 13:19:02.219098  \n",
       "4 2022-12-11 13:19:02.219098  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_immigration_df(df: pd.DataFrame, table_name):\n",
    "    df.to_sql(name=table_name, con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only to run these if want to load into db\n",
    "process_df(df_immi, 'immigration')\n",
    "process_df(df_city, 'city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore\n",
    "\n",
    "Here I first used pandas profiling package to get a taste of what the datasets look like, I exported the expolorary analysis to html files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas profiling, only run if the html analysis havn't been generated. It took a while to finish\n",
    "city_report = ProfileReport(df_city)\n",
    "city_report.to_file(output_file='city_profiling.html')\n",
    "\n",
    "immi_report = ProfileReport(df_immi)\n",
    "immi_report.to_file(output_file='immi_profiling.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get the total summation of all missing values in the DataFrame\n",
    "df_city.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12139874"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get the total summation of all missing values in the DataFrame\n",
    "df_immi.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city                       0\n",
       "state                      0\n",
       "median_age                 0\n",
       "male_population            3\n",
       "female_population          3\n",
       "total_population           0\n",
       "number_of_veterans        13\n",
       "foreign_born              13\n",
       "average_household_size    16\n",
       "state_code                 0\n",
       "race                       0\n",
       "count                      0\n",
       "updated_at                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cicid               0\n",
       "i94yr               0\n",
       "i94mon              0\n",
       "i94cit              0\n",
       "i94res              0\n",
       "i94port             0\n",
       "arrdate             0\n",
       "i94mode           239\n",
       "i94addr        152592\n",
       "depdate        142457\n",
       "i94bir            802\n",
       "i94visa             0\n",
       "count               0\n",
       "dtadfile            1\n",
       "visapost      1881250\n",
       "occup         3088187\n",
       "entdepa           238\n",
       "entdepd        138429\n",
       "entdepu       3095921\n",
       "matflag        138429\n",
       "biryear           802\n",
       "dtaddto           477\n",
       "gender         414269\n",
       "insnum        2982605\n",
       "airline         83627\n",
       "admnum              0\n",
       "fltno           19549\n",
       "visatype            0\n",
       "updated_at          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immi.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immi.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears in US city dataset, the missing values are relatively small; However, for immigration dataset there are significant amount of missing values need to be taken care of.\n",
    "\n",
    "There are no duplication in both datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing cleaning tasks here\n",
    "\n",
    "I will do data cleanning and data transformation in dbt, for that part of the code please find dbt_udacity_capstone directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ERD diagram url](https://dbdiagram.io/d/63957606bae3ed7c454608df)\n",
    "\n",
    "![ERD](images/udacity_data_engineer_capstone.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose Kimball's dimensional modelling methodology to model the input data.\n",
    "\n",
    "A dimension is basically a descriptive information about the facts, a fact indicates business measurements or business activities. I plan to model the input data based on the ERD diagram listed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use dbt to transform the raw data into facts and dimenstions, in between there is a staging area where I used to clean up the raw data.\n",
    "\n",
    "![architecture](images/architecture.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The data pipelines are built in dbt, please find the code in udacity_data_engineer_capstone directory.\n",
    " \n",
    " For workflow flow charts kindly find below.\n",
    " \n",
    " ![immi](images/immi_flowchart.png)\n",
    " \n",
    " ![demo](images/demographic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests are performed using dbt within './dbt_udacity_capstone/models/warehouse/schemas.yml' file.\n",
    "\n",
    "In total I used three generic tests.\n",
    "\n",
    "1. not_null: to test whether there is null value in given column.\n",
    "2. unique: check row uniqueness.\n",
    "3. relationship: check whether given column have relationship with another table.\n",
    "\n",
    "Kindly find the test results at below.\n",
    "\n",
    "![test](images/test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentations are generated using these commands in dbt directory.\n",
    "\n",
    "```\n",
    "dbt docs generate\n",
    "\n",
    "dbt docs serve\n",
    "```\n",
    "\n",
    "dbt docs generate â€” this command tells dbt to compile relevant information about dbt project and warehouse into manifest.json and catalog.json files respectively. \n",
    "\n",
    "Then, run dbt docs serve to use these .json files to populate a local website at port 8080."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please find the data dictionary for each table after started local server at port 8080.\n",
    "\n",
    "Below is the sample data dictionary for fct_immigration table.\n",
    "\n",
    "![dictionary_immi](images/dictionary_immi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project I built a ELT pipeline. The primary tools I used in this project are python, sql and dbt. I use python to script the extraction and loading parts of the ELT workflow, for the data transformation I mainly use sql and dbt in conjunction with airflow to do the workflow scheduling.\n",
    "\n",
    "Python, sql and airflow are the status quo within the data engineering world. While I choose dbt mainly because it brings the best practices from the software engineering world into data analysis including code version control, better sql code collaboration and testing. It avoids writing boilerplate DML and DDL by managing transactions, dropping tables, and managing schema changes under the hood for users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dbt modelling pipeline should have a daily running schedule to ingest new raw data coming to the database. The scheduling is performed by airflow. Purpose is to accurately reflect the latest change in raw data to build accurate facts and dimensions for data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data was incrased by 100x I will probably try to use airbyte instead of python pandas to ingest the data. Airbyte will use a change flag to do incremental update on the raw data table automatically, this will save me a lot of time ingesting data. Afterwards I will let airflow to trigger dbt to run the data transformation work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is a dashboard that must be updated on a daily basis by 7am every day, I will let Airflow to have a daily running job probably around 5am to trigger 'dbt run' & 'dbt test' commands to update the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the database is going to be accessed by 100+ people I will use nosql or cloud database instead of postgres. Cloud databases have better horizontal scaling capabilities which traditional RDBMS lacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
